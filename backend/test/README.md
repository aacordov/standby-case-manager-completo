# GuÃ­a de Testing - Standby Case Manager Backend

Esta guÃ­a documenta la estrategia de testing del backend del sistema de gestiÃ³n de casos.

## ğŸ“‹ Tabla de Contenidos

- [Estructura de Tests](#estructura-de-tests)
- [ConfiguraciÃ³n](#configuraciÃ³n)
- [EjecuciÃ³n de Tests](#ejecuciÃ³n-de-tests)
- [Cobertura](#cobertura)
- [Tipos de Tests](#tipos-de-tests)
- [Fixtures Disponibles](#fixtures-disponibles)
- [Buenas PrÃ¡cticas](#buenas-prÃ¡cticas)
- [Troubleshooting](#troubleshooting)

---

## ğŸ“ Estructura de Tests

```
backend/
â”œâ”€â”€ test/                        # âš ï¸ Nota: es 'test' sin 's'
â”‚   â”œâ”€â”€ conftest.py              # Fixtures compartidas
â”‚   â”œâ”€â”€ unit/                    # Tests unitarios
â”‚   â”‚   â”œâ”€â”€ test_auth_unit.py    # Tests de autenticaciÃ³n
â”‚   â”‚   â””â”€â”€ test_models.py       # Tests de modelos
â”‚   â””â”€â”€ integration/             # Tests de integraciÃ³n
â”‚       â”œâ”€â”€ test_auth_integration.py
â”‚       â”œâ”€â”€ test_cases_integration.py
â”‚       â””â”€â”€ test_users_integration.py
â”œâ”€â”€ pytest.ini                   # ConfiguraciÃ³n de pytest
â”œâ”€â”€ requirements.txt             # Dependencias principales
â””â”€â”€ requirements-test.txt        # Dependencias de testing
```

---

## âš™ï¸ ConfiguraciÃ³n

### 1. Crear Entorno Virtual

```bash
# Navegar al directorio backend
cd backend

# Crear entorno virtual
python3 -m venv .venv

# Activar entorno virtual
# En Linux/Mac:
source .venv/bin/activate

# En Windows:
.venv\Scripts\activate
```

### 2. Instalar Dependencias

```bash
# Instalar dependencias principales
pip install -r requirements.txt

# Instalar dependencias de testing
pip install -r requirements-test.txt
```

**Contenido de `requirements-test.txt`:**

```txt
# Testing Framework
pytest==8.0.0
pytest-asyncio==0.23.0
pytest-cov==4.1.0
pytest-xdist==3.5.0

# HTTP Testing
httpx==0.26.0

# Mocking
pytest-mock==3.12.0

# Database Testing
sqlalchemy-utils==0.41.1
```

### 3. ConfiguraciÃ³n de Pytest

El archivo `pytest.ini` configura el comportamiento de pytest:

```ini
[pytest]
testpaths = test
python_files = test_*.py
python_classes = Test*
python_functions = test_*
asyncio_mode = auto

# Markers
markers =
    unit: Tests unitarios
    integration: Tests de integraciÃ³n
    auth: Tests de autenticaciÃ³n
    cases: Tests de casos
    users: Tests de usuarios
    slow: Tests que toman mÃ¡s tiempo

# Coverage
addopts = 
    --cov=app
    --cov-report=term-missing
    --cov-report=html
    --cov-fail-under=80
    -v
```

---

## ğŸš€ EjecuciÃ³n de Tests

### Comandos BÃ¡sicos

```bash
# Activar entorno virtual (si no estÃ¡ activo)
source .venv/bin/activate

# Ejecutar todos los tests
pytest

# Modo verbose (mÃ¡s detalles)
pytest -v

# Modo muy verbose (mÃ¡ximo detalle)
pytest -vv

# Ver output de print statements
pytest -s
```

### Tests por Tipo

```bash
# Solo tests unitarios
pytest test/unit -v

# Solo tests de integraciÃ³n
pytest test/integration -v

# Tests por marca
pytest -m unit          # Solo unitarios
pytest -m integration   # Solo integraciÃ³n
pytest -m auth          # Solo autenticaciÃ³n
pytest -m cases         # Solo casos
```

### Tests EspecÃ­ficos

```bash
# Ejecutar un archivo especÃ­fico
pytest test/unit/test_auth_unit.py

# Ejecutar un archivo con verbose
pytest test/integration/test_cases_integration.py -v

# Ejecutar una clase especÃ­fica
pytest test/unit/test_auth_unit.py::TestPasswordHashing

# Ejecutar un test especÃ­fico
pytest test/unit/test_auth_unit.py::TestPasswordHashing::test_password_hash_generation
```

### Filtrar por Nombre

```bash
# Ejecutar tests que contengan "login" en el nombre
pytest -k "login"

# Ejecutar tests que NO contengan "slow"
pytest -k "not slow"

# Combinar filtros
pytest -k "login and not integration"
```

### Tests en Paralelo (MÃ¡s RÃ¡pido)

```bash
# Usar todos los CPUs disponibles
pytest -n auto

# Usar 4 procesos
pytest -n 4
```

---

## ğŸ“Š Cobertura

### Generar Reporte de Cobertura

```bash
# Reporte en terminal con lÃ­neas faltantes
pytest --cov=app --cov-report=term-missing

# Generar reporte HTML
pytest --cov=app --cov-report=html

# Generar ambos
pytest --cov=app --cov-report=term-missing --cov-report=html
```

### Ver Reporte HTML

```bash
# Abrir en navegador
# En Linux:
xdg-open htmlcov/index.html

# En Mac:
open htmlcov/index.html

# En Windows:
start htmlcov/index.html
```

### Cobertura por MÃ³dulo

```bash
# Solo cobertura de autenticaciÃ³n
pytest --cov=app.auth test/unit/test_auth_unit.py

# Solo cobertura de routers
pytest --cov=app.routers test/integration/
```

### Verificar Cobertura MÃ­nima

El proyecto requiere **80% de cobertura mÃ­nima** (configurado en `pytest.ini`).

```bash
# Falla si cobertura < 80%
pytest --cov=app --cov-fail-under=80
```

**Cobertura actual del proyecto: ~90%**

---

## ğŸ§ª Tipos de Tests

### Tests Unitarios (`test/unit/`)

Tests que prueban funciones y mÃ©todos individuales sin dependencias externas.

**CaracterÃ­sticas:**
- âœ… No requieren base de datos
- âœ… EjecuciÃ³n muy rÃ¡pida
- âœ… Prueban lÃ³gica de negocio aislada
- âœ… Marcados con `@pytest.mark.unit`

**Ejemplos:**
- ValidaciÃ³n de hash de contraseÃ±as
- CreaciÃ³n de tokens JWT
- ValidaciÃ³n de modelos Pydantic
- Funciones de utilidad

**Ejemplo de test unitario:**

```python
import pytest
from app.auth import verify_password, get_password_hash

@pytest.mark.unit
def test_password_hash_verification():
    """Test que el hash de contraseÃ±a puede ser verificado"""
    password = "test_password_123"
    hashed = get_password_hash(password)
    
    assert verify_password(password, hashed)
    assert not verify_password("wrong_password", hashed)
```

### Tests de IntegraciÃ³n (`test/integration/`)

Tests que prueban endpoints completos con todas sus dependencias.

**CaracterÃ­sticas:**
- âœ… Usan base de datos SQLite en memoria
- âœ… Prueban flujos completos
- âœ… Verifican interacciÃ³n entre componentes
- âœ… Marcados con `@pytest.mark.integration`

**Ejemplos:**
- Login y autenticaciÃ³n completa
- CreaciÃ³n y actualizaciÃ³n de casos
- GestiÃ³n de usuarios con permisos
- Upload y descarga de archivos

**Ejemplo de test de integraciÃ³n:**

```python
import pytest
from httpx import AsyncClient

@pytest.mark.integration
@pytest.mark.asyncio
async def test_create_case_success(
    client: AsyncClient,
    admin_headers: dict
):
    """Test de creaciÃ³n de caso exitosa"""
    case_data = {
        "title": "Test Case",
        "description": "Test Description",
        "priority": "alta",
        "status": "abierto",
        "case_type": "incidente"
    }
    
    response = await client.post(
        "/cases/",
        json=case_data,
        headers=admin_headers
    )
    
    assert response.status_code == 201
    data = response.json()
    assert data["title"] == case_data["title"]
    assert "id" in data
```

---

## ğŸ¯ Fixtures Disponibles

Las fixtures estÃ¡n definidas en `test/conftest.py` y estÃ¡n disponibles para todos los tests.

### Fixtures de Base de Datos

```python
@pytest.fixture
def db_session():
    """SesiÃ³n de base de datos para tests"""
    # Base de datos SQLite en memoria
    # Se limpia automÃ¡ticamente despuÃ©s de cada test
```

```python
@pytest.fixture
def client():
    """Cliente HTTP de prueba (AsyncClient)"""
    # Cliente configurado para hacer requests a la API
```

### Fixtures de Usuarios

```python
@pytest.fixture
def admin_user():
    """Usuario con rol ADMIN"""
    # Usuario pre-creado para tests
```

```python
@pytest.fixture
def ingreso_user():
    """Usuario con rol INGRESO"""
```

```python
@pytest.fixture
def consulta_user():
    """Usuario con rol CONSULTA"""
```

### Fixtures de AutenticaciÃ³n

```python
@pytest.fixture
def admin_token():
    """Token JWT vÃ¡lido para admin"""
```

```python
@pytest.fixture
def admin_headers():
    """Headers HTTP con autenticaciÃ³n de admin"""
    # Incluye: {"Authorization": "Bearer <token>"}
```

### Fixtures de Casos

```python
@pytest.fixture
def sample_case():
    """Un caso de ejemplo"""
```

```python
@pytest.fixture
def multiple_cases():
    """Lista de 10 casos con diferentes estados"""
```

### Ejemplo de Uso

```python
@pytest.mark.integration
@pytest.mark.asyncio
async def test_get_case_by_id(
    client: AsyncClient,
    admin_headers: dict,
    sample_case: Case
):
    response = await client.get(
        f"/cases/{sample_case.id}",
        headers=admin_headers
    )
    
    assert response.status_code == 200
    data = response.json()
    assert data["id"] == sample_case.id
```

---

## ğŸ¨ Marcadores (Markers)

Los tests estÃ¡n organizados con marcadores para ejecuciÃ³n selectiva:

```python
@pytest.mark.unit          # Test unitario
@pytest.mark.integration   # Test de integraciÃ³n
@pytest.mark.auth          # Test de autenticaciÃ³n
@pytest.mark.cases         # Test de casos
@pytest.mark.users         # Test de usuarios
@pytest.mark.slow          # Test que toma tiempo
```

### Combinar Marcadores

```bash
# Tests unitarios de autenticaciÃ³n
pytest -m "unit and auth"

# Tests de integraciÃ³n de casos
pytest -m "integration and cases"

# Todos excepto los lentos
pytest -m "not slow"

# Unitarios o de autenticaciÃ³n
pytest -m "unit or auth"
```

---

## âœ… Buenas PrÃ¡cticas

### 1. Nombres Descriptivos

```python
# âœ… BIEN
def test_admin_can_create_case_with_all_fields():
    ...

# âŒ MAL
def test_case_1():
    ...
```

### 2. Tests Independientes

```python
# âœ… BIEN - Cada test crea sus propios datos
@pytest.mark.integration
async def test_delete_case(client, admin_headers):
    # Crear caso para este test
    case = await create_test_case()
    
    # Eliminar
    response = await client.delete(f"/cases/{case.id}", headers=admin_headers)
    assert response.status_code == 200

# âŒ MAL - Depende de datos de otro test
@pytest.mark.integration
async def test_delete_case_2(client, admin_headers):
    # Asume que existe caso con id=1
    response = await client.delete("/cases/1", headers=admin_headers)
    ...
```

### 3. Usar Fixtures

```python
# âœ… BIEN - Usa fixture
@pytest.mark.integration
async def test_with_fixture(client, sample_case):
    response = await client.get(f"/cases/{sample_case.id}")
    ...

# âŒ MAL - Crea datos manualmente
@pytest.mark.integration
async def test_without_fixture(client):
    # Mucha configuraciÃ³n repetitiva...
    user = User(...)
    db.add(user)
    case = Case(...)
    db.add(case)
    ...
```

### 4. Tests AtÃ³micos

```python
# âœ… BIEN - Un test, una verificaciÃ³n principal
def test_password_hash_is_valid():
    password = "test123"
    hashed = get_password_hash(password)
    assert verify_password(password, hashed)

# âŒ MAL - Muchas verificaciones no relacionadas
def test_everything():
    # Test de hash
    ...
    # Test de JWT
    ...
    # Test de validaciÃ³n
    ...
```

### 5. Documentar Tests Complejos

```python
@pytest.mark.integration
async def test_complex_workflow(client, admin_headers):
    """
    Test del flujo completo de creaciÃ³n y actualizaciÃ³n de caso.
    
    Flujo:
    1. Crear caso como admin
    2. Verificar que se creÃ³ correctamente
    3. Actualizar el caso
    4. Verificar actualizaciÃ³n
    5. Verificar que el historial se guardÃ³
    """
    # ImplementaciÃ³n...
```

---

## ğŸ” Debugging

### Usar Python Debugger (pdb)

```bash
# Detener en fallos
pytest --pdb

# Detener en el primer fallo
pytest -x --pdb
```

### Ver Traceback Completo

```bash
pytest --tb=long
```

### Ejecutar Solo Tests que Fallaron

```bash
# Ãšltimo test fallido
pytest --lf

# Tests fallidos primero, luego el resto
pytest --ff
```

### Aumentar Verbosidad

```bash
# Ver cada test ejecutado
pytest -v

# Ver detalles de cada assert
pytest -vv
```

---

## ğŸ› Troubleshooting

### Error: "ModuleNotFoundError: No module named 'app'"

**SoluciÃ³n:**

```bash
# Asegurarse de estar en el directorio correcto
cd backend

# Verificar que el entorno virtual estÃ¡ activo
source .venv/bin/activate

# Verificar que las dependencias estÃ¡n instaladas
pip list | grep fastapi
```

### Error: "Database is locked"

**Causa:** Los tests usan SQLite en memoria, esto no deberÃ­a ocurrir.

**SoluciÃ³n:**
- Verificar que las fixtures usan `async with` correctamente
- Reiniciar pytest

### Tests Muy Lentos

**SoluciÃ³n:**

```bash
# Instalar pytest-xdist
pip install pytest-xdist

# Ejecutar en paralelo
pytest -n auto
```

### Error: "Event loop is closed"

**Causa:** Problema con pytest-asyncio

**SoluciÃ³n:**

```bash
# Verificar configuraciÃ³n en pytest.ini
asyncio_mode = auto

# O usar el marcador en cada test
@pytest.mark.asyncio
async def test_something():
    ...
```

---

## ğŸ“š Recursos Adicionales

- [Pytest Documentation](https://docs.pytest.org/)
- [pytest-asyncio](https://pytest-asyncio.readthedocs.io/)
- [HTTPX Testing](https://www.python-httpx.org/advanced/#testing)
- [FastAPI Testing](https://fastapi.tiangolo.com/tutorial/testing/)
- [Coverage.py](https://coverage.readthedocs.io/)

---

## ğŸ¤ Contribuir con Tests

Al agregar nuevas funcionalidades:

1. âœ… Escribir tests unitarios para lÃ³gica de negocio
2. âœ… Escribir tests de integraciÃ³n para endpoints
3. âœ… Mantener cobertura >80%
4. âœ… Ejecutar todos los tests antes de PR
5. âœ… Actualizar esta documentaciÃ³n si es necesario

---

## ğŸ“ Contacto

Para dudas sobre testing:

- **Allan CÃ³rdova**: [aacordov@gmail.com](mailto:aacordov@gmail.com)
- **JosÃ© Briones**: [josmbrio@gmail.com](mailto:josmbrio@gmail.com)
- **Larry SÃ¡nchez**: [lajasanc@gmail.com](mailto:lajasanc@gmail.com)
- **Ronny Ortiz**: [ronny.ortiz.54@hotmail.com](mailto:ronny.ortiz.54@hotmail.com)

---

**Ãšltima actualizaciÃ³n:** Enero 2026  
**VersiÃ³n:** 2.0.0
